{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99368756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kpaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f70857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motivation_and_Goals__Part_3 Expecting value: line 1 column 1 (char 0)\n",
      "Motivation_and_Goals__Part_4 Expecting value: line 1 column 1 (char 0)\n",
      "Psychiatrist_Reacts Expecting value: line 1 column 1 (char 0)\n",
      "Therapist_Reacts Expecting value: line 1 column 1 (char 0)\n",
      "Therapist_Talks Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "documents_path = os.path.join('..', 'data', 'subtitles')\n",
    "documents = os.listdir(documents_path)\n",
    "\n",
    "texts = []\n",
    "titles = []\n",
    "for document in documents:\n",
    "    try:\n",
    "        doc_path = os.path.join('..', 'data', 'subtitles', document)\n",
    "\n",
    "        with open(doc_path, 'r') as f:\n",
    "            doc_json = json.load(f)\n",
    "\n",
    "        # get sentences\n",
    "        sents = pd.DataFrame(doc_json)['text'].to_numpy()\n",
    "\n",
    "        # get the document and truncate empty spaces\n",
    "        doc = ' '.join(sents)\n",
    "        doc = re.sub('\\s+', ' ', doc)\n",
    "        \n",
    "        texts.append(doc)\n",
    "        titles.append(document[:-5])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(document, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc3dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + ['like', 'right', '__', '_connector_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31655741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdc7890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('money needed guide', 0.594),\n",
       " ('afford donate extra', 0.5379),\n",
       " ('able afford donate', 0.5318),\n",
       " ('afford donate', 0.5264),\n",
       " ('help want money', 0.4854)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que = \"\"\"I'm older and am lucky to have a job that pays me enough to live. Without details, Healthygamer has helped me at my lowest point in life and helped me to seek help. I am significantly improving my daily living. (Dr.K, thank you for what you do if you read this.) With this, I want to give back, and while my mind was wandering, I had a thought. There is \"Dr.K's guide to mental health,\" which is an excellent toolâ€”knowing that most people ( not all) may not have access to the money needed for the guide. Due to going to college, high school, pandemic, etc. Three years ago, I wouldn't have been able to afford the guide, which led me to an idea. Could there be a way for people to buy an extra copy for someone randomly who wouldn't be able to afford it or donate an extra couple of bucks to help out? Maybe this is already in place, and I missed it, but I figured I would share my idea. Money was a big reason I couldn't seek help for a while, and I wouldn't want money to get in the way of a helpful tool\"\"\"\n",
    "kw_model.extract_keywords(que, keyphrase_ngram_range=(1, 1), stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ba7d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('misogyny', 0.3664),\n",
       " ('misogynistic', 0.2967),\n",
       " ('insulting', 0.2911),\n",
       " ('posts', 0.2829),\n",
       " ('misogynist', 0.2799)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que = texts[1]\n",
    "kw_model.extract_keywords(que, keyphrase_ngram_range=(1, 1), stop_words=stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git-conda38",
   "language": "python",
   "name": "git-conda38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
